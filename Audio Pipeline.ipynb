{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5720713",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-family:Georgia; font-weight:bold; \">Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.layers import (\n",
    "    Attention,\n",
    "    Bidirectional,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Reshape,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e485d5",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-family:Georgia; font-weight:bold; \">Constants and Global Variables</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af40173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = pd.read_csv(\"Collected Datasets/audio.csv\")\n",
    "audio.info()\n",
    "\n",
    "EMOTIONS = [\"happiness\", \"neutral\", \"sadness\", \"anger\", \"fear\"]\n",
    "MODELS = [layers.LSTM, layers.GRU, layers.SimpleRNN]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.array(EMOTIONS)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1660d7",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-family:Georgia; font-weight:bold; \">Cleaning and Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mfccStrToArray(mfccStr):\n",
    "    vals = np.array(list(map(float, mfccStr.split(\",\"))))\n",
    "    return vals.reshape(13, vals.size // 13)\n",
    "\n",
    "\n",
    "def _mfccArrayToStr(mfccArr):\n",
    "    return \",\".join(map(str, mfccArr.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56be3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(data, f=\"train\"):\n",
    "    if f == \"train\":\n",
    "        data = scaler.fit_transform(data)\n",
    "    else:\n",
    "        data = scaler.transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09fe048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, f=\"train\"):\n",
    "    if f == \"train\":\n",
    "        data = encoder.fit_transform(data)\n",
    "    else:\n",
    "        data = encoder.transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3701e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractMFCC(filePath):\n",
    "    y, sr = librosa.load(filePath, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfccFlat = mfcc.flatten()\n",
    "    return \",\".join(map(str, mfccFlat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0498ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroPadding(data, maxLen=120):\n",
    "    data[\"mfcc\"] = data[\"mfcc\"].apply(_mfccStrToArray)\n",
    "\n",
    "    def _padToMaxLen(mfccArr):\n",
    "        if mfccArr.shape[1] < maxLen:\n",
    "            pad_width = maxLen - mfccArr.shape[1]\n",
    "            mfccArr = np.pad(mfccArr, ((0, 0), (0, pad_width)), mode=\"constant\")\n",
    "        return mfccArr\n",
    "\n",
    "    data[\"mfcc\"] = data[\"mfcc\"].apply(_padToMaxLen)\n",
    "    data[\"mfcc\"] = data[\"mfcc\"].apply(_mfccArrayToStr)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(yTrue, yPred):\n",
    "    print(classification_report(yTrue, yPred, target_names=EMOTIONS))\n",
    "\n",
    "    accuracy = accuracy_score(yTrue, yPred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    precision = precision_score(yTrue, yPred, average=\"weighted\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "    recall = recall_score(yTrue, yPred, average=\"weighted\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "    f1 = f1_score(yTrue, yPred, average=\"weighted\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(yTrue, yPred)\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(shape, modelType):\n",
    "    print(f\"\\nBuilding model with {modelType.__name__}\")\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=shape))\n",
    "\n",
    "    model.add(Reshape((shape[0], shape[1], 1)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Reshape((-1, model.output_shape[-1] * model.output_shape[-2])))\n",
    "\n",
    "    model.add(Bidirectional(modelType(120, return_sequences=True)))\n",
    "\n",
    "    def _attention(x):\n",
    "        attn_layer = Attention()\n",
    "        attn_output = attn_layer([x, x])\n",
    "        return tf.reduce_mean(attn_output, axis=1)\n",
    "\n",
    "    model.add(layers.Lambda(_attention))\n",
    "\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(len(EMOTIONS), activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, xTrain, yTrain, xVal, yVal):\n",
    "    print(f\"\\nTraining {model.layers[0].__class__.__name__} model...\")\n",
    "    model.fit(\n",
    "        xTrain,\n",
    "        yTrain,\n",
    "        validation_data=(xVal, yVal),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=True,\n",
    "    )\n",
    "    predictions = model.predict(xVal)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    trueLabels = np.argmax(yVal, axis=1)\n",
    "    evaluateModel(trueLabels, predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareAudioModels(xTrain, yTrain, xVal, yVal):\n",
    "    bestModel = None\n",
    "    for model in MODELS:\n",
    "        model = buildModel(xTrain.shape[1:], model)\n",
    "        model = train(model, xTrain, yTrain, xVal, yVal)\n",
    "\n",
    "        if model.layers[0].__class__.__name__ == \"SimpleRNN\":\n",
    "            bestModel = model\n",
    "\n",
    "    return bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = zeroPadding(audio)\n",
    "\n",
    "features = np.array([_mfccStrToArray(mfcc) for mfcc in audio[\"mfcc\"]])\n",
    "features = np.transpose(features, (0, 2, 1))\n",
    "\n",
    "targets = encode(audio[\"Emotion\"], f=\"train\")\n",
    "targets = to_categorical(targets)\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(\n",
    "    features, targets, test_size=0.2, stratify=targets, random_state=42\n",
    ")\n",
    "\n",
    "samplesTrain, timeSteps, numFeatures = xTrain.shape\n",
    "xTrainFlat = xTrain.reshape(-1, numFeatures)\n",
    "xValFlat = xVal.reshape(-1, numFeatures)\n",
    "\n",
    "xTrainScaled = scale(xTrainFlat, f=\"train\")\n",
    "xValScaled = scale(xValFlat, f=\"test\")\n",
    "\n",
    "xTrainScaled = xTrainScaled.reshape(samplesTrain, timeSteps, numFeatures)\n",
    "xValScaled = xValScaled.reshape(xVal.shape[0], timeSteps, numFeatures)\n",
    "\n",
    "bestModel = compareAudioModels(xTrainScaled, yTrain, xValScaled, yVal)\n",
    "bestModel.save(\"bestAudioEmotionModel.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
