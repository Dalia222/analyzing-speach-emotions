{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0270cf",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-family:Georgia; font-weight:bold; \">HMM for Emotion Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb826f83",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26585dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from hmmlearn import hmm\n",
    "from collections import Counter\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43720f5a",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8988988",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedData = pd.read_csv('./Preprocessed Data/preprocessed_text.csv')\n",
    "print(f\"Loaded preprocessed data with shape: {preprocessedData.shape}\")\n",
    "print(f\"Columns: {preprocessedData.columns.tolist()}\")\n",
    "\n",
    "preprocessedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0fdbb7",
   "metadata": {},
   "source": [
    "## Constants and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9842762",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = ['happiness', 'neutral', 'sadness', 'anger', 'fear']\n",
    "MAX_WORDS = 10000  \n",
    "MAX_SEQUENCE_LENGTH = 100  \n",
    "EMBEDDING_DIM = 300\n",
    "NUM_COMPONENTS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.array(EMOTIONS)\n",
    "\n",
    "if 'label' not in preprocessedData.columns:\n",
    "    preprocessedData['label'] = encoder.transform(preprocessedData['Emotion'])\n",
    "\n",
    "emotionCounts = preprocessedData['Emotion'].value_counts()\n",
    "print(\"Emotion distribution in dataset:\")\n",
    "print(emotionCounts)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=emotionCounts.index, y=emotionCounts.values)\n",
    "plt.title('Emotion Distribution in Dataset')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc52e31b",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text.split()\n",
    "\n",
    "print(\"Tokenizing texts...\")\n",
    "tokenizedTexts = [cleanText(text) for text in tqdm(preprocessedData['Text'])]\n",
    "\n",
    "seqLengths = [len(tokens) for tokens in tokenizedTexts]\n",
    "print(f\"Average sequence length: {np.mean(seqLengths):.2f}\")\n",
    "print(f\"Max sequence length: {np.max(seqLengths)}\")\n",
    "print(f\"90th percentile sequence length: {np.percentile(seqLengths, 90):.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(seqLengths, bins=50)\n",
    "plt.title('Distribution of Text Length')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Count')\n",
    "plt.axvline(x=MAX_SEQUENCE_LENGTH, color='r', linestyle='--', label=f'Max Length: {MAX_SEQUENCE_LENGTH}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d900eaa",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Word2Vec model on our dataset...\")\n",
    "w2vModel = Word2Vec(sentences=tokenizedTexts, vector_size=EMBEDDING_DIM, window=5, min_count=1, workers=4)\n",
    "print(f\"Word2Vec model trained. Vocabulary size: {len(w2vModel.wv.key_to_index)}\")\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=MAX_WORDS)\n",
    "bowMatrix = vectorizer.fit_transform([' '.join(tokens) for tokens in tokenizedTexts])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Bag of Words matrix shape: {bowMatrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac3a2c4",
   "metadata": {},
   "source": [
    "## Text to Sequence Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33962ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToSequenceTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, wordVectors, sequenceLength=MAX_SEQUENCE_LENGTH):\n",
    "        self.wordVectors = wordVectors\n",
    "        self.sequenceLength = sequenceLength\n",
    "        self.vectorSize = wordVectors.vector_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        sequences = np.zeros((len(X), self.sequenceLength, self.vectorSize))\n",
    "        \n",
    "        for i, text in enumerate(X):\n",
    "            tokens = cleanText(text)\n",
    "            for j, token in enumerate(tokens):\n",
    "                if j >= self.sequenceLength:\n",
    "                    break\n",
    "                    \n",
    "                if token in self.wordVectors:\n",
    "                    sequences[i, j] = self.wordVectors[token]\n",
    "        \n",
    "        return sequences\n",
    "\n",
    "def textToSequence(texts, wordVectors, maxLength=MAX_SEQUENCE_LENGTH):\n",
    "    sequences = np.zeros((len(texts), maxLength, wordVectors.vector_size))\n",
    "    \n",
    "    for i, tokens in enumerate(texts):\n",
    "        for j, token in enumerate(tokens):\n",
    "            if j >= maxLength:\n",
    "                break\n",
    "                \n",
    "            if token in wordVectors:\n",
    "                sequences[i, j] = wordVectors[token]\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "textSequences = textToSequence(tokenizedTexts, w2vModel.wv)\n",
    "print(f\"Text sequences shape: {textSequences.shape}\")\n",
    "\n",
    "seqMeans = np.mean(textSequences, axis=1)\n",
    "print(f\"Sequence means shape: {seqMeans.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0107e",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d36639",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = preprocessedData['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    seqMeans, labels, test_size=0.2, stratify=labels, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a3562e",
   "metadata": {},
   "source": [
    "## HMM Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmmModels = {}\n",
    "\n",
    "for emotion in EMOTIONS:\n",
    "    emotionIdx = list(encoder.classes_).index(emotion)\n",
    "    X_emotion = X_train_scaled[y_train == emotionIdx]\n",
    "    \n",
    "    if len(X_emotion) > 0:\n",
    "        model = hmm.GaussianHMM(\n",
    "            n_components=NUM_COMPONENTS, \n",
    "            covariance_type=\"full\", \n",
    "            n_iter=100,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        model.fit(X_emotion)\n",
    "        hmmModels[emotion] = model\n",
    "        print(f\"Trained HMM for {emotion} on {len(X_emotion)} samples\")\n",
    "    else:\n",
    "        print(f\"No samples for {emotion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a65c8",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97730f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictWithHMM(X, models):\n",
    "    results = []\n",
    "    for x in X:\n",
    "        best_score = -np.inf\n",
    "        best_label = None\n",
    "        x = x.reshape(1, -1)\n",
    "        \n",
    "        for emotion, model in models.items():\n",
    "            try:\n",
    "                score = model.score(x)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_label = emotion\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        results.append(best_label if best_label else EMOTIONS[0])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluateModels(X, y_true, models):\n",
    "    y_pred = predictWithHMM(X, models)\n",
    "    y_pred_encoded = encoder.transform(y_pred)\n",
    "    y_true_names = encoder.inverse_transform(y_true)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred_encoded)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true_names, y_pred, target_names=EMOTIONS))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred_encoded)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=EMOTIONS, yticklabels=EMOTIONS)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, y_pred_encoded\n",
    "\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_accuracy, val_preds = evaluateModels(X_val_scaled, y_val, hmmModels)\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_accuracy, test_preds = evaluateModels(X_test_scaled, y_test, hmmModels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4724f1",
   "metadata": {},
   "source": [
    "## Lexicon Score Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ba808",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreColumns = [col for col in preprocessedData.columns if col.endswith('Score')]\n",
    "if scoreColumns:\n",
    "    testIndices = np.random.choice(range(len(preprocessedData)), size=len(y_test), replace=False)\n",
    "    testScores = preprocessedData.iloc[testIndices][scoreColumns].values\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i, emotion in enumerate(EMOTIONS):\n",
    "        emotionIndices = np.where(y_test == i)[0]\n",
    "        if len(emotionIndices) > 0:\n",
    "            avgScores = np.mean(testScores[emotionIndices], axis=0)\n",
    "            \n",
    "            plt.subplot(2, 3, i+1)\n",
    "            plt.bar(range(len(scoreColumns)), avgScores)\n",
    "            plt.title(f'Avg Lexicon Scores for {emotion}')\n",
    "            plt.xticks(range(len(scoreColumns)), [s.replace('Score', '') for s in scoreColumns], rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07299629",
   "metadata": {},
   "source": [
    "## Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b84b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictEmotion(text, models, w2vModel, scaler):\n",
    "    cleanedText = cleanText(text)\n",
    "    \n",
    "    sequenceVectors = np.zeros((len(cleanedText), w2vModel.vector_size))\n",
    "    for i, token in enumerate(cleanedText):\n",
    "        if token in w2vModel.wv:\n",
    "            sequenceVectors[i] = w2vModel.wv[token]\n",
    "    \n",
    "    if len(sequenceVectors) > 0:\n",
    "        meanVector = np.mean(sequenceVectors, axis=0)\n",
    "        scaledVector = scaler.transform(meanVector.reshape(1, -1))\n",
    "        \n",
    "        bestScore = -np.inf\n",
    "        bestEmotion = None\n",
    "        \n",
    "        for emotion, model in models.items():\n",
    "            try:\n",
    "                score = model.score(scaledVector)\n",
    "                if score > bestScore:\n",
    "                    bestScore = score\n",
    "                    bestEmotion = emotion\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return bestEmotion if bestEmotion else EMOTIONS[0]\n",
    "    else:\n",
    "        return EMOTIONS[0]\n",
    "\n",
    "testExamples = [\n",
    "    \"I am so happy today, everything is wonderful!\",\n",
    "    \"I feel so sad and depressed after what happened.\",\n",
    "    \"I'm absolutely furious about the way they treated me.\",\n",
    "    \"I'm really scared about what might happen next.\",\n",
    "    \"It's just another normal day, nothing special.\"\n",
    "]\n",
    "\n",
    "for text in testExamples:\n",
    "    emotion = predictEmotion(text, hmmModels, w2vModel.wv, scaler)\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"Predicted emotion: {emotion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444b4be",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmmMetrics = {\n",
    "    'accuracy': test_accuracy,\n",
    "    'model_name': 'HMM with Word2Vec'\n",
    "}\n",
    "\n",
    "print(f\"HMM with Word2Vec Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28f3b4",
   "metadata": {},
   "source": [
    "## Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "textLengths = [len(text.split()) for text in preprocessedData['Text'].iloc[testIndices]]\n",
    "\n",
    "bins = [0, 10, 20, 30, 40, 50, 70, 100, 1000]\n",
    "lengthBins = pd.cut(textLengths, bins=bins)\n",
    "\n",
    "correctPredictions = (test_preds == y_test)\n",
    "dfResults = pd.DataFrame({\n",
    "    'textLength': textLengths,\n",
    "    'lengthBin': lengthBins,\n",
    "    'correct': correctPredictions\n",
    "})\n",
    "\n",
    "accuracyByLength = dfResults.groupby('lengthBin')['correct'].mean()\n",
    "samplesByLength = dfResults.groupby('lengthBin').size()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "accuracyByLength.plot(kind='bar')\n",
    "plt.title('HMM Model Accuracy by Text Length')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "samplesByLength.plot(kind='bar')\n",
    "plt.title('Number of Samples by Text Length')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7095310",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('hmm_emotion_models.pkl', 'wb') as f:\n",
    "    pickle.dump(hmmModels, f)\n",
    "\n",
    "w2vModel.save(\"hmm_word2vec.model\")\n",
    "\n",
    "with open('hmm_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Saved HMM models, Word2Vec model, and scaler\")\n",
    "\n",
    "print(\"\\n=== HMM with Word2Vec Model Summary ===\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Number of HMM states: {NUM_COMPONENTS}\")\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
