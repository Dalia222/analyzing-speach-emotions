{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccc5d0b",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-family:Georgia; font-weight:bold; \">BiLSTM with Word2Vec</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41bb810",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f3088d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\__init__.py:133\u001b[0m\n\u001b[0;32m    125\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mPopen \u001b[38;5;241m=\u001b[39m _fake_Popen\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollocations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator, memoize\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatstruct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\collocations.py:36\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_itertools\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# these two unused imports are referenced in collocations.doctest\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m     BigramAssocMeasures,\n\u001b[0;32m     38\u001b[0m     ContingencyMeasures,\n\u001b[0;32m     39\u001b[0m     QuadgramAssocMeasures,\n\u001b[0;32m     40\u001b[0m     TrigramAssocMeasures,\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspearman\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ranks_from_scores, spearman_correlation\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\metrics\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magreement\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnnotationTask\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m align\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massociation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     BigramAssocMeasures,\n\u001b[0;32m     20\u001b[0m     ContingencyMeasures,\n\u001b[0;32m     21\u001b[0m     NgramAssocMeasures,\n\u001b[0;32m     22\u001b[0m     QuadgramAssocMeasures,\n\u001b[0;32m     23\u001b[0m     TrigramAssocMeasures,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfusionmatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrix\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     binary_distance,\n\u001b[0;32m     28\u001b[0m     custom_distance,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     presence,\n\u001b[0;32m     36\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\metrics\\association.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m _SMALL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-20\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fisher_exact\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfisher_exact\u001b[39m(\u001b[38;5;241m*\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs):\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\scipy\\stats\\__init__.py:608\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m \n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    607\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 608\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\scipy\\stats\\_stats_py.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyVersion\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suppress_warnings\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _measurements\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, MapWrapper, _get_nan,\n\u001b[0;32m     42\u001b[0m                               rng_integers, _rename_parameter, _contains_nan)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\scipy\\spatial\\__init__.py:110\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=============================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m   QhullError\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_qhull\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\scipy\\spatial\\_kdtree.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright Anne M. Archibald 2008\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Released under the scipy license\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cKDTree, cKDTreeNode\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance_p\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRectangle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDTree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminkowski_distance_p\u001b[39m(x, y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[1;32m_ckdtree.pyx:1\u001b[0m, in \u001b[0;36minit scipy.spatial._ckdtree\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb5337",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d91e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = pd.read_csv('./Preprocessed Data/preprocessed_text.csv')\n",
    "print(f\"Loaded preprocessed data with shape: {preprocessed_data.shape}\")\n",
    "print(f\"Columns: {preprocessed_data.columns.tolist()}\")\n",
    "\n",
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212fa784",
   "metadata": {},
   "source": [
    "## Constants and Global Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a3f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = ['happiness', 'neutral', 'sadness', 'anger', 'fear']\n",
    "MAX_WORDS = 10000  \n",
    "MAX_SEQUENCE_LENGTH = 100  \n",
    "EMBEDDING_DIM = 300  \n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.array(EMOTIONS)\n",
    "\n",
    "if 'label' not in preprocessed_data.columns:\n",
    "    preprocessed_data['label'] = encoder.transform(preprocessed_data['Emotion'])\n",
    "\n",
    "emotion_counts = preprocessed_data['Emotion'].value_counts()\n",
    "print(\"Emotion distribution in dataset:\")\n",
    "print(emotion_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=emotion_counts.index, y=emotion_counts.values)\n",
    "plt.title('Emotion Distribution in Dataset')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b4a47",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text.split()\n",
    "\n",
    "print(\"Tokenizing texts...\")\n",
    "tokenized_texts = [clean_text(text) for text in tqdm(preprocessed_data['Text'])]\n",
    "\n",
    "# Calculate sequence length statistics\n",
    "seq_lengths = [len(tokens) for tokens in tokenized_texts]\n",
    "print(f\"Average sequence length: {np.mean(seq_lengths):.2f}\")\n",
    "print(f\"Max sequence length: {np.max(seq_lengths)}\")\n",
    "print(f\"90th percentile sequence length: {np.percentile(seq_lengths, 90):.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(seq_lengths, bins=50)\n",
    "plt.title('Distribution of Text Length')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Count')\n",
    "plt.axvline(x=MAX_SEQUENCE_LENGTH, color='r', linestyle='--', label=f'Max Length: {MAX_SEQUENCE_LENGTH}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504503e1",
   "metadata": {},
   "source": [
    "## Word2Vec Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f9997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Train Word2Vec on our dataset\n",
    "print(\"Training Word2Vec model on our dataset...\")\n",
    "w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=EMBEDDING_DIM, window=5, min_count=1, workers=4)\n",
    "print(f\"Word2Vec model trained. Vocabulary size: {len(w2v_model.wv.key_to_index)}\")\n",
    "\n",
    "# Option 2: Load pre-trained Word2Vec (Google News)\n",
    "# Uncomment to use pre-trained model instead\n",
    "# print(\"Loading pre-trained Word2Vec model...\")\n",
    "# w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "# print(f\"Pre-trained Word2Vec model loaded. Vocabulary size: {len(w2v_model.key_to_index)}\")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(tokenized_texts)\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"Found {len(word_index)} unique tokens\")\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(tokenized_texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((min(MAX_WORDS, len(word_index) + 1), EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = w2v_model.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f6d223",
   "metadata": {},
   "source": [
    "## Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35aaaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(preprocessed_data['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d0a28c",
   "metadata": {},
   "source": [
    "## BiLSTM Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9752067",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    input_dim=min(MAX_WORDS, len(word_index) + 1),\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False  \n",
    "))\n",
    "\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(EMOTIONS), activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9f989",
   "metadata": {},
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='best_bilstm_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc744bcb",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "y_pred_emotions = encoder.inverse_transform(y_pred)\n",
    "y_true_emotions = encoder.inverse_transform(y_true)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_emotions, y_pred_emotions, target_names=EMOTIONS))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=EMOTIONS, yticklabels=EMOTIONS)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b169c",
   "metadata": {},
   "source": [
    "## Lexicon Score Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674cf203",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_columns = [col for col in preprocessed_data.columns if col.endswith('Score')]\n",
    "if score_columns:\n",
    "    test_indices = np.random.choice(range(len(preprocessed_data)), size=len(y_test), replace=False)\n",
    "    test_scores = preprocessed_data.iloc[test_indices][score_columns].values\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i, emotion in enumerate(EMOTIONS):\n",
    "        emotion_indices = np.where(y_true == i)[0]\n",
    "        \n",
    "        avg_scores = np.mean(test_scores[emotion_indices], axis=0)\n",
    "        \n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.bar(range(len(score_columns)), avg_scores)\n",
    "        plt.title(f'Avg Lexicon Scores for {emotion}')\n",
    "        plt.xticks(range(len(score_columns)), [s.replace('Score', '') for s in score_columns], rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dea1500",
   "metadata": {},
   "source": [
    "## Model for Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c80b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(text, tokenizer, model, encoder):\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    \n",
    "    prediction = model.predict(padded)[0]\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    predicted_emotion = encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "    scores = {emotion: float(score) for emotion, score in zip(EMOTIONS, prediction)}\n",
    "    \n",
    "    return {\n",
    "        'emotion': predicted_emotion,\n",
    "        'confidence': float(np.max(prediction)),\n",
    "        'scores': scores\n",
    "    }\n",
    "\n",
    "test_examples = [\n",
    "    \"I am so happy today, everything is wonderful!\",\n",
    "    \"I feel so sad and depressed after what happened.\",\n",
    "    \"I'm absolutely furious about the way they treated me.\",\n",
    "    \"I'm really scared about what might happen next.\",\n",
    "    \"It's just another normal day, nothing special.\"\n",
    "]\n",
    "\n",
    "for text in test_examples:\n",
    "    result = predict_emotion(text, tokenizer, model, encoder)\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"Predicted emotion: {result['emotion']} (confidence: {result['confidence']:.2f})\")\n",
    "    print(\"Scores for all emotions:\")\n",
    "    for emotion, score in sorted(result['scores'].items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {emotion}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec657aaf",
   "metadata": {},
   "source": [
    "## Model Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0158fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_metrics = {\n",
    "    'accuracy': test_accuracy,\n",
    "    'model_name': 'BiLSTM with Word2Vec'\n",
    "}\n",
    "\n",
    "print(f\"BiLSTM with Word2Vec Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a8ad3",
   "metadata": {},
   "source": [
    "## Text Length Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7025ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lengths = [len(text.split()) for text in preprocessed_data['Text'].iloc[test_indices]]\n",
    "\n",
    "bins = [0, 10, 20, 30, 40, 50, 70, 100, 1000]\n",
    "length_bins = pd.cut(text_lengths, bins=bins)\n",
    "\n",
    "correct_predictions = (y_pred == y_true)\n",
    "df_results = pd.DataFrame({\n",
    "    'text_length': text_lengths,\n",
    "    'length_bin': length_bins,\n",
    "    'correct': correct_predictions\n",
    "})\n",
    "\n",
    "accuracy_by_length = df_results.groupby('length_bin')['correct'].mean()\n",
    "samples_by_length = df_results.groupby('length_bin').size()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "accuracy_by_length.plot(kind='bar')\n",
    "plt.title('BiLSTM Model Accuracy by Text Length')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "samples_by_length.plot(kind='bar')\n",
    "plt.title('Number of Samples by Text Length')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8740c42",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e4e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"bilstm_emotion_model.h5\")\n",
    "\n",
    "import pickle\n",
    "with open('bilstm_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")\n",
    "print(\"\\n=== BiLSTM with Word2Vec Model Summary ===\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Model size: {sum(np.prod(v.get_shape().as_list()) for v in model.trainable_variables):,} trainable parameters\")\n",
    "print(\"Embedding type: Word2Vec\")\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(\"\\nStrengths:\")\n",
    "print(\"- Captures sequential nature of text\")\n",
    "print(\"- Uses both forward and backward context\")\n",
    "print(\"- Leverages semantic relationships from Word2Vec\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"- Compare with DistilRoBERTa and other models\")\n",
    "print(\"- Fine-tune hyperparameters\")\n",
    "print(\"- Try attention mechanisms\")\n",
    "print(\"- Explore ensemble methods\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
