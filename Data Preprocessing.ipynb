{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5720713",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-family:Georgia; font-weight:bold; \">Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b938b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = api.load(\"word2vec-google-news-300\")  \n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e485d5",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-family:Georgia; font-weight:bold; \">Constants and Global Variables</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af40173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv('Collected Datasets/text.csv')\n",
    "\n",
    "EMOTIONS = ['happiness', 'neutral', 'sadness', 'anger', 'fear', ]\n",
    "MAPPER = {emotion: [] for emotion in EMOTIONS}\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.array(EMOTIONS)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Emotional Lexicons'''\n",
    "wordMap = pd.read_csv('wordMap.csv')\n",
    "for emotion in wordMap.columns.to_list():    \n",
    "    if emotion.lower() in EMOTIONS: MAPPER[emotion.lower()] = wordMap[emotion].to_list()\n",
    "    if emotion in ['Calm', 'Boredom']: MAPPER['neutral'] += wordMap[emotion].to_list()\n",
    "    if emotion in ['Excitement', 'Pride']: MAPPER['happiness'] += wordMap[emotion].to_list()\n",
    "    if emotion in ['Disgust', 'Frustration', 'Contempt']: MAPPER['anger'] += wordMap[emotion].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Contractions'''\n",
    "with open(\"Common English Contractions/contractions.json\" , 'r') as file:\n",
    "    contractions = json.load(file)\n",
    "    \n",
    "contractions = pd.DataFrame(list(contractions.items()), columns=[\"Contraction\", \"Meaning\"])\n",
    "contractions = pd.concat([contractions, pd.read_csv(\"Common English Contractions/contractions.csv\")], ignore_index=True)\n",
    "contractions.drop_duplicates(inplace=True)\n",
    "contractions[\"Contraction\"] = contractions[\"Contraction\"].str.lower()\n",
    "contractions[\"Meaning\"] = contractions[\"Meaning\"].str.lower()\n",
    "contractions.info()\n",
    "contractions = contractions.set_index(\"Contraction\").to_dict()[\"Meaning\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1660d7",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-family:Georgia; font-weight:bold; \">Cleaning and Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f53154",
   "metadata": {},
   "source": [
    "<pre>\n",
    "- All characters are lowercase\n",
    "- No non alphabetic characters or numbers\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56be3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(data, f=\"train\"):\n",
    "    if f == \"train\":\n",
    "        data = scaler.fit_transform(data)\n",
    "    else:\n",
    "        data = scaler.transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09fe048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, f=\"train\"):\n",
    "    if f == \"train\":\n",
    "        data = encoder.fit_transform(data)\n",
    "    else:\n",
    "        data = encoder.transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd03783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandContractions(data):\n",
    "    count = 0\n",
    "    for contraction, meaning in contractions.items():\n",
    "        count += data['Text'].apply(lambda line: len(re.findall(rf'\\b{contraction}\\b', line))).sum()\n",
    "        data['Text'] = data['Text'].apply(\n",
    "            lambda line: re.sub(rf'\\b{contraction}\\b', meaning, line)\n",
    "            )\n",
    "        \n",
    "    print(\"Number of contractions removed:\", count)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe849037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNER(data):\n",
    "    data['Entities'] = data['Text'].apply(\n",
    "    lambda text: [(ent.text, ent.label_) for ent in nlp(text).ents]\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexiconScore(data):\n",
    "    for emotion, keywords in MAPPER.items():\n",
    "        scores = []\n",
    "        for text in data['Text']:\n",
    "            words = text.lower().split()\n",
    "            counter = Counter(words)\n",
    "            totalWords = len(words)\n",
    "            score = sum(counter[word] for word in keywords)\n",
    "            normalizedScore = score / totalWords if totalWords > 0 else 0.0\n",
    "            scores.append(normalizedScore)\n",
    "        data[f\"{emotion}Score\"] = scores\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf979e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWordRemoval(data):\n",
    "    data[\"Text\"] = data[\"Text\"].apply(\n",
    "        lambda removeStopWords: [word for word in removeStopWords if word not in STOP_WORDS]\n",
    "        )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57dea787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    data[\"Text\"] = data[\"Text\"].apply(\n",
    "        lambda text: word_tokenize(text)\n",
    "        )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc2cb5",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-family:Georgia; font-weight:bold; \">Saving Preprocessed Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "411bc8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of contractions removed: 3094\n"
     ]
    }
   ],
   "source": [
    "text = expandContractions(text)\n",
    "text = tokenize(text)\n",
    "text = stopWordRemoval(text)\n",
    "text[\"Text\"] = text[\"Text\"].apply(lambda words: \" \".join(words))  # Rejoin for scoring\n",
    "text = lexiconScore(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5056f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed text saved to 'preprocessed_text.csv'\n"
     ]
    }
   ],
   "source": [
    "text.to_csv(\"preprocessed_text.csv\", index=False)\n",
    "print(\"Preprocessed text saved to 'preprocessed_text.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4c808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
